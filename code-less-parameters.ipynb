{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1180d78b-06f7-4c17-9109-3e00716f10a1",
   "metadata": {},
   "source": [
    "# 1.Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a96b91-05cd-4e88-b3f8-68a4d41587e4",
   "metadata": {},
   "source": [
    "Loading and Initial Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e98ae472-089c-4034-9110-dc9784d17cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing training data...\n",
      "Processing training file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\0degC/589_Mixed1.csv\n",
      "Processing training file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\0degC/589_Mixed2.csv\n",
      "Processing training file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\0degC/590_Mixed4.csv\n",
      "Processing training file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\0degC/590_Mixed5.csv\n",
      "Processing training file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\0degC/590_Mixed6.csv\n",
      "Processing training file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\0degC/590_Mixed8.csv\n",
      "Processing training file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\10degC/567_Mixed1.csv\n",
      "Processing training file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\10degC/567_Mixed2.csv\n",
      "Processing training file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\10degC/571_Mixed4.csv\n",
      "Processing training file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\10degC/571_Mixed5.csv\n",
      "Processing training file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\10degC/571_Mixed6.csv\n",
      "Processing training file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\10degC/571_Mixed8.csv\n",
      "Processing training file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\25degC/551_Mixed1.csv\n",
      "Processing training file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\25degC/551_Mixed2.csv\n",
      "Processing training file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\25degC/552_Mixed4.csv\n",
      "Processing training file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\25degC/552_Mixed5.csv\n",
      "Processing training file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\25degC/552_Mixed6.csv\n",
      "Processing training file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\25degC/552_Mixed8.csv\n",
      "\n",
      "Training data loaded and concatenated successfully!\n",
      "Total training data shape: (1222903, 17)\n",
      "\n",
      "First 5 rows of concatenated training DataFrame:\n",
      "               Time Stamp  Step Status     Prog Time     Step Time  Cycle  \\\n",
      "0  11/30/2018 11:20:56 PM    54  TABLE  28:28:10.083  00:00:01.613      1   \n",
      "1  11/30/2018 11:20:56 PM    54  TABLE  28:28:10.187  00:00:01.717      1   \n",
      "2  11/30/2018 11:20:56 PM    54  TABLE  28:28:10.282  00:00:01.812      1   \n",
      "3  11/30/2018 11:20:56 PM    54  TABLE  28:28:10.383  00:00:01.913      1   \n",
      "4  11/30/2018 11:20:57 PM    54  TABLE  28:28:10.485  00:00:02.015      1   \n",
      "\n",
      "   Cycle Level       Procedure  Voltage  Current  Temperature  Capacity  \\\n",
      "0            1  LG_HG2_CyclesA  4.18380 -0.07662     -0.73611  -0.00000   \n",
      "1            1  LG_HG2_CyclesA  4.18228 -0.08939     -0.73611  -0.00001   \n",
      "2            1  LG_HG2_CyclesA  4.18110 -0.09450     -0.73611  -0.00001   \n",
      "3            1  LG_HG2_CyclesA  4.18026 -0.09450     -0.73611  -0.00001   \n",
      "4            1  LG_HG2_CyclesA  4.17975 -0.09450     -0.73611  -0.00001   \n",
      "\n",
      "    WhAccu  Cnt  SoC Capacity  SoC Percentage     Original_File  \n",
      "0 -0.00001  6.0       2.34975        1.000000  0degC/589_Mixed1  \n",
      "1 -0.00002  6.0       2.34974        0.999996  0degC/589_Mixed1  \n",
      "2 -0.00003  6.0       2.34974        0.999996  0degC/589_Mixed1  \n",
      "3 -0.00005  6.0       2.34974        0.999996  0degC/589_Mixed1  \n",
      "4 -0.00006  6.0       2.34974        0.999996  0degC/589_Mixed1  \n",
      "\n",
      "Training DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1222903 entries, 0 to 1222902\n",
      "Data columns (total 17 columns):\n",
      " #   Column          Non-Null Count    Dtype  \n",
      "---  ------          --------------    -----  \n",
      " 0   Time Stamp      1222903 non-null  object \n",
      " 1   Step            1222903 non-null  int64  \n",
      " 2   Status          1222903 non-null  object \n",
      " 3   Prog Time       1222903 non-null  object \n",
      " 4   Step Time       1222903 non-null  object \n",
      " 5   Cycle           1222903 non-null  int64  \n",
      " 6   Cycle Level     1222903 non-null  int64  \n",
      " 7   Procedure       1222903 non-null  object \n",
      " 8   Voltage         1222903 non-null  float64\n",
      " 9   Current         1222903 non-null  float64\n",
      " 10  Temperature     1222903 non-null  float64\n",
      " 11  Capacity        1222903 non-null  float64\n",
      " 12  WhAccu          1222903 non-null  float64\n",
      " 13  Cnt             1222903 non-null  float64\n",
      " 14  SoC Capacity    1222903 non-null  float64\n",
      " 15  SoC Percentage  1222903 non-null  float64\n",
      " 16  Original_File   1222903 non-null  object \n",
      "dtypes: float64(8), int64(3), object(6)\n",
      "memory usage: 158.6+ MB\n",
      "\n",
      "Loading and preprocessing testing data...\n",
      "Processing testing file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\0degC/589_LA92.csv\n",
      "Processing testing file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\0degC/589_UDDS.csv\n",
      "Processing testing file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\0degC/589_US06.csv\n",
      "Processing testing file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\0degC/590_Mixed7.csv\n",
      "Processing testing file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\10degC/576_UDDS.csv\n",
      "Processing testing file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\10degC/567_US06.csv\n",
      "Processing testing file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\10degC/571_Mixed7.csv\n",
      "Processing testing file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\25degC/551_LA92.csv\n",
      "Processing testing file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\25degC/551_UDDS.csv\n",
      "Processing testing file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\25degC/551_US06.csv\n",
      "Processing testing file: C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion\\25degC/552_Mixed7.csv\n",
      "\n",
      "Testing data loaded and concatenated successfully!\n",
      "Total testing data shape: (882077, 17)\n",
      "\n",
      "First 5 rows of concatenated testing DataFrame:\n",
      "              Time Stamp  Step Status     Prog Time     Step Time  Cycle  \\\n",
      "0  11/30/2018 1:24:32 PM    40  TABLE  18:31:45.970  00:00:02.220      1   \n",
      "1  11/30/2018 1:24:32 PM    40  TABLE  18:31:46.067  00:00:02.317      1   \n",
      "2  11/30/2018 1:24:32 PM    40  TABLE  18:31:46.168  00:00:02.418      1   \n",
      "3  11/30/2018 1:24:32 PM    40  TABLE  18:31:46.265  00:00:02.515      1   \n",
      "4  11/30/2018 1:24:32 PM    40  TABLE  18:31:46.368  00:00:02.618      1   \n",
      "\n",
      "   Cycle Level       Procedure  Voltage  Current  Temperature  Capacity  \\\n",
      "0            1  LG_HG2_CyclesA  4.18413 -0.07918     -0.73611  -0.00000   \n",
      "1            1  LG_HG2_CyclesA  4.18278 -0.08939     -0.73611  -0.00000   \n",
      "2            1  LG_HG2_CyclesA  4.18160 -0.09450     -0.73611  -0.00001   \n",
      "3            1  LG_HG2_CyclesA  4.18076 -0.09450     -0.73611  -0.00001   \n",
      "4            1  LG_HG2_CyclesA  4.18009 -0.09450     -0.63095  -0.00001   \n",
      "\n",
      "    WhAccu  Cnt  SoC Capacity  SoC Percentage   Original_File  \n",
      "0 -0.00001  4.0       2.34988        1.000000  0degC/589_LA92  \n",
      "1 -0.00002  4.0       2.34988        1.000000  0degC/589_LA92  \n",
      "2 -0.00003  4.0       2.34987        0.999996  0degC/589_LA92  \n",
      "3 -0.00004  4.0       2.34987        0.999996  0degC/589_LA92  \n",
      "4 -0.00005  4.0       2.34987        0.999996  0degC/589_LA92  \n",
      "\n",
      "Testing DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 882077 entries, 0 to 882076\n",
      "Data columns (total 17 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   Time Stamp      882077 non-null  object \n",
      " 1   Step            882077 non-null  int64  \n",
      " 2   Status          882077 non-null  object \n",
      " 3   Prog Time       882077 non-null  object \n",
      " 4   Step Time       882077 non-null  object \n",
      " 5   Cycle           882077 non-null  int64  \n",
      " 6   Cycle Level     882077 non-null  int64  \n",
      " 7   Procedure       882077 non-null  object \n",
      " 8   Voltage         882077 non-null  float64\n",
      " 9   Current         882077 non-null  float64\n",
      " 10  Temperature     882077 non-null  float64\n",
      " 11  Capacity        882077 non-null  float64\n",
      " 12  WhAccu          882077 non-null  float64\n",
      " 13  Cnt             882077 non-null  float64\n",
      " 14  SoC Capacity    882077 non-null  float64\n",
      " 15  SoC Percentage  882077 non-null  float64\n",
      " 16  Original_File   882077 non-null  object \n",
      "dtypes: float64(8), int64(3), object(6)\n",
      "memory usage: 114.4+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "dataset_base_path = 'C:/Users/sahar/Documents/sut/lpd/project/Dataset_Li-ion' \n",
    "\n",
    "# Define the lists of train and test files\n",
    "train_files = [\n",
    "    '0degC/589_Mixed1', '0degC/589_Mixed2', '0degC/590_Mixed4',\n",
    "    '0degC/590_Mixed5', '0degC/590_Mixed6', '0degC/590_Mixed8',\n",
    "    '10degC/567_Mixed1', '10degC/567_Mixed2', '10degC/571_Mixed4',\n",
    "    '10degC/571_Mixed5', '10degC/571_Mixed6', '10degC/571_Mixed8',\n",
    "    '25degC/551_Mixed1', '25degC/551_Mixed2',\n",
    "    '25degC/552_Mixed4', '25degC/552_Mixed5', '25degC/552_Mixed6',\n",
    "    '25degC/552_Mixed8',\n",
    "]\n",
    "test_files = [\n",
    "    '0degC/589_LA92', '0degC/589_UDDS', '0degC/589_US06', '0degC/590_Mixed7',\n",
    "    '10degC/576_UDDS', '10degC/567_US06', '10degC/571_Mixed7',\n",
    "    '25degC/551_LA92', '25degC/551_UDDS', '25degC/551_US06', '25degC/552_Mixed7',\n",
    "]\n",
    "\n",
    "# Define common column names\n",
    "column_names = [\n",
    "    'Time Stamp','Step','Status','Prog Time','Step Time','Cycle',\n",
    "    'Cycle Level','Procedure','Voltage','Current','Temperature','Capacity','WhAccu','Cnt','Empty'\n",
    "]\n",
    "\n",
    "# --- Function to Load and Preprocess a Single File ---\n",
    "def load_and_preprocess_file(file_path):\n",
    "    \"\"\"\n",
    "    Loads a single battery data file, applies column names, filters by status,\n",
    "    and calculates SoC.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the CSV, skipping the header rows\n",
    "        df = pd.read_csv(file_path, skiprows=30)\n",
    "        \n",
    "        # Assign the predefined column names\n",
    "        df.columns = column_names\n",
    "        \n",
    "        # Remove the 'Empty' column if it exists\n",
    "        if 'Empty' in df.columns:\n",
    "            df = df.drop(columns=['Empty'])\n",
    "            \n",
    "        # Filter rows based on 'Status' column\n",
    "        df = df[(df[\"Status\"] == \"TABLE\") | (df[\"Status\"] == \"DCH\")]\n",
    "        \n",
    "        # Calculate SoC Capacity and SoC Percentage\n",
    "        \n",
    "        # Ensure 'Capacity' column is numeric, convert if necessary\n",
    "        df['Capacity'] = pd.to_numeric(df['Capacity'], errors='coerce')\n",
    "        df.dropna(subset=['Capacity'], inplace=True) # Drop rows where Capacity conversion failed\n",
    "        \n",
    "        if not df.empty:\n",
    "            max_discharge = abs(df[\"Capacity\"].min()) # Get maximum discharge from this file\n",
    "            df[\"SoC Capacity\"] = max_discharge + df[\"Capacity\"]\n",
    "            # Ensure no division by zero if max_discharge is 0\n",
    "            df[\"SoC Percentage\"] = df[\"SoC Capacity\"] / df[\"SoC Capacity\"].max() if df[\"SoC Capacity\"].max() != 0 else 0\n",
    "            df[\"SoC Percentage\"] = df[\"SoC Percentage\"].clip(lower=0, upper=1) # Ensure SoC is between 0 and 1\n",
    "        else:\n",
    "            print(f\"Warning: No valid data after filtering for {file_path}. Skipping SoC calculation.\")\n",
    "            df[\"SoC Capacity\"] = 0\n",
    "            df[\"SoC Percentage\"] = 0\n",
    "            \n",
    "        return df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}. Please check your base path and file lists.\")\n",
    "        return pd.DataFrame() # Return empty DataFrame on error\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {file_path}: {e}\")\n",
    "        return pd.DataFrame() # Return empty DataFrame on error\n",
    "\n",
    "# --- Load and Preprocess Training Data ---\n",
    "print(\"Loading and preprocessing training data...\")\n",
    "train_dataframes = []\n",
    "for file_name in train_files:\n",
    "    full_path = os.path.join(dataset_base_path, file_name + '.csv') # Append .csv\n",
    "    print(f\"Processing training file: {full_path}\")\n",
    "    df = load_and_preprocess_file(full_path)\n",
    "    if not df.empty:\n",
    "        df['Original_File'] = file_name # Keep track of original file\n",
    "        train_dataframes.append(df)\n",
    "\n",
    "if train_dataframes:\n",
    "    train_df = pd.concat(train_dataframes, ignore_index=True)\n",
    "    print(\"\\nTraining data loaded and concatenated successfully!\")\n",
    "    print(f\"Total training data shape: {train_df.shape}\")\n",
    "    print(\"\\nFirst 5 rows of concatenated training DataFrame:\")\n",
    "    print(train_df.head())\n",
    "    print(\"\\nTraining DataFrame Info:\")\n",
    "    train_df.info()\n",
    "else:\n",
    "    train_df = pd.DataFrame()\n",
    "    print(\"\\nNo training data loaded. Check paths and file lists.\")\n",
    "\n",
    "# --- Load and Preprocess Testing Data ---\n",
    "print(\"\\nLoading and preprocessing testing data...\")\n",
    "test_dataframes = []\n",
    "for file_name in test_files:\n",
    "    full_path = os.path.join(dataset_base_path, file_name + '.csv') # Append .csv\n",
    "    print(f\"Processing testing file: {full_path}\")\n",
    "    df = load_and_preprocess_file(full_path)\n",
    "    if not df.empty:\n",
    "        df['Original_File'] = file_name # Keep track of original file\n",
    "        test_dataframes.append(df)\n",
    "\n",
    "if test_dataframes:\n",
    "    test_df = pd.concat(test_dataframes, ignore_index=True)\n",
    "    print(\"\\nTesting data loaded and concatenated successfully!\")\n",
    "    print(f\"Total testing data shape: {test_df.shape}\")\n",
    "    print(\"\\nFirst 5 rows of concatenated testing DataFrame:\")\n",
    "    print(test_df.head())\n",
    "    print(\"\\nTesting DataFrame Info:\")\n",
    "    test_df.info()\n",
    "else:\n",
    "    test_df = pd.DataFrame()\n",
    "    print(\"\\nNo testing data loaded. Check paths and file lists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3a1560-7281-4f1e-af36-0a62ff0b7a61",
   "metadata": {},
   "source": [
    " Feature Engineering & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c623073-86b7-48a2-a1a1-1392860e8403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Feature Engineering...\n",
      "  Features engineered for DataFrame. New columns: V_avg, I_avg, Power.\n",
      "  Features engineered for DataFrame. New columns: V_avg, I_avg, Power.\n",
      "\n",
      "Starting Normalization (Min-Max Scaling)...\n",
      "  Training features scaled. Shape: (1222903, 6)\n",
      "  Testing features scaled. Shape: (882077, 6)\n",
      "  Training target shape: (1222903,)\n",
      "  Testing target shape: (882077,)\n",
      "\n",
      "First 5 rows of scaled training features (X_train_scaled):\n",
      "[[0.74769365 0.96123602 0.00387602 0.98822162 0.72474142 0.66467244]\n",
      " [0.74716384 0.96018574 0.00387602 0.98767909 0.72445214 0.66397179]\n",
      " [0.74695183 0.95937038 0.00387602 0.98721746 0.72427854 0.66369227]\n",
      " [0.74695183 0.95878996 0.00387602 0.98683674 0.72419174 0.66369332]\n",
      " [0.74695183 0.95843756 0.00387602 0.98653549 0.72413966 0.66369395]]\n",
      "\n",
      "First 5 values of training target (y_train):\n",
      "[1.         0.99999574 0.99999574 0.99999574 0.99999574]\n",
      "\n",
      "Feature Engineering and Normalization complete!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "\n",
    "print(\"Starting Feature Engineering...\")\n",
    "\n",
    "# Define a rolling window size for average calculations.\n",
    "# The paper doesn't specify a window size, so we'll start with a reasonable value.\n",
    "rolling_window_size = 30 # Example: 30 data points \n",
    "\n",
    "def apply_feature_engineering(df):\n",
    "    \"\"\"Applies feature engineering steps to a DataFrame.\"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # Ensure Voltage and Current are numeric\n",
    "    df['Voltage'] = pd.to_numeric(df['Voltage'], errors='coerce')\n",
    "    df['Current'] = pd.to_numeric(df['Current'], errors='coerce')\n",
    "    \n",
    "    # Drop rows where Voltage or Current became NaN due to coercion\n",
    "    df.dropna(subset=['Voltage', 'Current'], inplace=True)\n",
    "\n",
    "    if df.empty: # Check again after dropping NaNs\n",
    "        return df\n",
    "        \n",
    "    # Calculate Average Voltage (V_avg) using a rolling mean\n",
    "    # We apply this per original file to avoid averaging across different cycles/files\n",
    "    df['V_avg'] = df.groupby('Original_File')['Voltage'].transform(lambda x: x.rolling(window=rolling_window_size, min_periods=1).mean())\n",
    "\n",
    "    # Calculate Average Current (I_avg) using a rolling mean\n",
    "    df['I_avg'] = df.groupby('Original_File')['Current'].transform(lambda x: x.rolling(window=rolling_window_size, min_periods=1).mean())\n",
    "\n",
    "    # Calculate Power (Watts) = Voltage * Current\n",
    "    df['Power'] = df['Voltage'] * df['Current']\n",
    "\n",
    "    print(f\"  Features engineered for DataFrame. New columns: V_avg, I_avg, Power.\")\n",
    "    return df\n",
    "\n",
    "train_df = apply_feature_engineering(train_df.copy())\n",
    "test_df = apply_feature_engineering(test_df.copy())\n",
    "\n",
    "# --- Select Features for Deep Learning Model ---\n",
    "# Based on the paper's input parameters: Current, Voltage, Temperature, Average Voltage, Average Current\n",
    "# We also calculated Power, which can be a valuable feature.\n",
    "# We will use 'SoC Percentage' as our target (y).\n",
    "\n",
    "# The features list\n",
    "features = ['Current', 'Voltage', 'Temperature', 'V_avg', 'I_avg', 'Power']\n",
    "target = 'SoC Percentage'\n",
    "\n",
    "# Check if all features exist in the DataFrames after engineering\n",
    "missing_train_features = [f for f in features if f not in train_df.columns]\n",
    "missing_test_features = [f for f in features if f not in test_df.columns]\n",
    "\n",
    "if missing_train_features:\n",
    "    print(f\"Warning: Missing features in train_df: {missing_train_features}. Please check feature engineering.\")\n",
    "if missing_test_features:\n",
    "    print(f\"Warning: Missing features in test_df: {missing_test_features}. Please check feature engineering.\")\n",
    "\n",
    "# --- Normalization (Min-Max Scaling) ---\n",
    "# We fit the scaler ONLY on the training data to prevent data leakage from the test set.\n",
    "print(\"\\nStarting Normalization (Min-Max Scaling)...\")\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the training data features and transform them\n",
    "# We convert to numpy arrays because MinMaxScaler works well with them\n",
    "X_train_scaled = scaler.fit_transform(train_df[features].values)\n",
    "y_train = train_df[target].values\n",
    "\n",
    "# Transform the test data features using the scaler fitted on training data\n",
    "X_test_scaled = scaler.transform(test_df[features].values)\n",
    "y_test = test_df[target].values\n",
    "\n",
    "print(f\"  Training features scaled. Shape: {X_train_scaled.shape}\")\n",
    "print(f\"  Testing features scaled. Shape: {X_test_scaled.shape}\")\n",
    "print(f\"  Training target shape: {y_train.shape}\")\n",
    "print(f\"  Testing target shape: {y_test.shape}\")\n",
    "\n",
    "print(\"\\nFirst 5 rows of scaled training features (X_train_scaled):\")\n",
    "print(X_train_scaled[:5])\n",
    "print(\"\\nFirst 5 values of training target (y_train):\")\n",
    "print(y_train[:5])\n",
    "\n",
    "print(\"\\nFeature Engineering and Normalization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb5defb-b4c4-4cdd-b62b-d1b0535a7dc7",
   "metadata": {},
   "source": [
    "Sequence Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d338989a-8459-4e09-be18-9b54eb13a1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Sequence Creation with sequence_length = 60...\n",
      "  Training sequences created. X_train_sequences shape: (1222843, 60, 6)\n",
      "  Training target sequences created. y_train_sequences shape: (1222843,)\n",
      "  Testing sequences created. X_test_sequences shape: (882017, 60, 6)\n",
      "  Testing target sequences created. y_test_sequences shape: (882017,)\n",
      "\n",
      "Example of the first training input sequence (X_train_sequences[0]):\n",
      "[[0.74769365 0.96123602 0.00387602 0.98822162 0.72474142 0.66467244]\n",
      " [0.74716384 0.96018574 0.00387602 0.98767909 0.72445214 0.66397179]\n",
      " [0.74695183 0.95937038 0.00387602 0.98721746 0.72427854 0.66369227]\n",
      " [0.74695183 0.95878996 0.00387602 0.98683674 0.72419174 0.66369332]\n",
      " [0.74695183 0.95843756 0.00387602 0.98653549 0.72413966 0.66369395]\n",
      " [0.74684562 0.9579746  0.00387602 0.98625495 0.72408561 0.66355413]\n",
      " [0.74684562 0.95773967 0.00387602 0.98601988 0.72404701 0.66355456]\n",
      " [0.74695183 0.95750473 0.00387602 0.98581325 0.72403255 0.66369563]\n",
      " [0.74695183 0.9572698  0.00387602 0.98562556 0.7240213  0.66369605]\n",
      " [0.74695183 0.95715924 0.00387602 0.98546399 0.72401231 0.66369625]\n",
      " [0.74684562 0.95704178 0.00387602 0.98532077 0.7239944  0.66355585]\n",
      " [0.74695183 0.95692431 0.00387602 0.9851913  0.72398915 0.66369667]\n",
      " [0.74684562 0.95680684 0.00387602 0.98507242 0.72397578 0.66355628]\n",
      " [0.74684562 0.95668938 0.00387602 0.98496185 0.72396432 0.6635565 ]\n",
      " [0.74695183 0.95657191 0.00387602 0.98485793 0.72396213 0.66369731]\n",
      " [0.74695183 0.95657191 0.00387602 0.984767   0.7239602  0.66369731]\n",
      " [0.74695183 0.95645444 0.00387602 0.98467963 0.7239585  0.66369752]\n",
      " [0.74695183 0.95645444 0.00387602 0.98460197 0.723957   0.66369752]\n",
      " [0.74695183 0.95645444 0.00387602 0.98453249 0.72395565 0.66369752]\n",
      " [0.74695183 0.95634389 0.00387602 0.98446424 0.72395443 0.66369772]\n",
      " [0.74684562 0.95634389 0.00387602 0.98440249 0.72394781 0.66355714]\n",
      " [0.74684562 0.95634389 0.00387602 0.98434636 0.72394179 0.66355714]\n",
      " [0.74684562 0.95634389 0.00387602 0.9842951  0.72393629 0.66355714]\n",
      " [0.74684562 0.95622642 0.00387602 0.98424306 0.72393125 0.66355735]\n",
      " [0.74684562 0.95622642 0.00387602 0.98419519 0.72392662 0.66355735]\n",
      " [0.74684562 0.95622642 0.00387602 0.98415099 0.72392234 0.66355735]\n",
      " [0.74684562 0.95622642 0.00387602 0.98411007 0.72391838 0.66355735]\n",
      " [0.74684562 0.95622642 0.00387602 0.98407208 0.7239147  0.66355735]\n",
      " [0.74695183 0.95610895 0.00387602 0.98403252 0.72391527 0.66369814]\n",
      " [0.74695183 0.95610895 0.00387602 0.98399559 0.72391581 0.66369814]\n",
      " [0.74695183 0.95610895 0.00387602 0.98381903 0.72388881 0.66369814]\n",
      " [0.74695183 0.95610895 0.00387602 0.98367864 0.72388109 0.66369814]\n",
      " [0.74684562 0.95610895 0.00387602 0.98356633 0.72387722 0.66355757]\n",
      " [0.74684562 0.95610895 0.00387602 0.983474   0.72387336 0.66355757]\n",
      " [0.74695183 0.95599149 0.00387602 0.98338977 0.72387336 0.66369835]\n",
      " [0.74695183 0.95599149 0.00387602 0.98332148 0.72387722 0.66369835]\n",
      " [0.74695183 0.95599149 0.00387602 0.98326127 0.72388109 0.66369835]\n",
      " [0.74684562 0.95599149 0.00387602 0.98320916 0.72387722 0.66355779]\n",
      " [0.74684562 0.95599149 0.00387602 0.98316514 0.72387336 0.66355779]\n",
      " [0.74684562 0.95599149 0.00387602 0.98312493 0.72386949 0.66355779]\n",
      " [0.74695183 0.95599149 0.00387602 0.98308876 0.72387336 0.66369835]\n",
      " [0.74684562 0.95599149 0.00387602 0.98305663 0.72386949 0.66355779]\n",
      " [0.74684562 0.95587402 0.00387602 0.98302451 0.72386949 0.66355801]\n",
      " [0.74695183 0.95587402 0.00387602 0.98299643 0.72387336 0.66369856]\n",
      " [0.74695183 0.95587402 0.00387602 0.9829724  0.72387336 0.66369856]\n",
      " [0.74695183 0.95587402 0.00387602 0.98294837 0.72387336 0.66369856]\n",
      " [0.74684562 0.95587402 0.00387602 0.98292838 0.72386949 0.66355801]\n",
      " [0.74695183 0.95587402 0.00387602 0.98290839 0.72386949 0.66369856]\n",
      " [0.74684562 0.95587402 0.00387602 0.9828884  0.72386562 0.66355801]\n",
      " [0.74695183 0.95587402 0.00387602 0.98287222 0.72386562 0.66369856]\n",
      " [0.74684562 0.95575655 0.00387602 0.982852   0.72386562 0.66355822]\n",
      " [0.74695183 0.95575655 0.00387602 0.98283177 0.72386949 0.66369877]\n",
      " [0.74695183 0.95575655 0.00387602 0.98281154 0.72387336 0.66369877]\n",
      " [0.74695183 0.95575655 0.00387602 0.98279536 0.72387722 0.66369877]\n",
      " [0.74695183 0.95575655 0.00387602 0.98277918 0.72388109 0.66369877]\n",
      " [0.74695183 0.95575655 0.00387602 0.982763   0.72388495 0.66369877]\n",
      " [0.74695183 0.95575655 0.00387602 0.98274682 0.72388882 0.66369877]\n",
      " [0.74684562 0.95575655 0.00387602 0.98273064 0.72388882 0.66355822]\n",
      " [0.74684562 0.95575655 0.00387602 0.98271851 0.72388495 0.66355822]\n",
      " [0.74695183 0.95575655 0.00387602 0.98270637 0.72388495 0.66369877]]\n",
      "\n",
      "Example of the first training target value (y_train_sequences[0]):\n",
      "0.9999319076497499\n",
      "\n",
      "Sequence Creation complete!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- Configuration for Sequence Creation ---\n",
    "# Define the sequence length (number of time steps the model looks back)\n",
    "sequence_length = 60\n",
    "\n",
    "print(f\"Starting Sequence Creation with sequence_length = {sequence_length}...\")\n",
    "\n",
    "def create_sequences(X, y, seq_length):\n",
    "    \"\"\"\n",
    "    Creates sequences from input features (X) and target (y) for RNN models.\n",
    "    Each input sequence will have 'seq_length' time steps.\n",
    "    The target (y) for a given sequence is typically the value at the end of that sequence.\n",
    "    \"\"\"\n",
    "    xs, ys = [], []\n",
    "    # Iterate through the data, creating sequences\n",
    "    for i in range(len(X) - seq_length):\n",
    "        # The input sequence is X from current position 'i' up to 'i + seq_length'\n",
    "        x_seq = X[i:(i + seq_length)]\n",
    "        # The target for this sequence is the y value at the end of the sequence\n",
    "        y_val = y[i + seq_length]\n",
    "        \n",
    "        xs.append(x_seq)\n",
    "        ys.append(y_val)\n",
    "    \n",
    "    # Convert lists to NumPy arrays\n",
    "    # X_sequences will have shape (num_samples, seq_length, num_features)\n",
    "    # y_sequences will have shape (num_samples, seq_length, num_features)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "# Create sequences for training data\n",
    "X_train_sequences, y_train_sequences = create_sequences(X_train_scaled, y_train, sequence_length)\n",
    "\n",
    "# Create sequences for testing data\n",
    "X_test_sequences, y_test_sequences = create_sequences(X_test_scaled, y_test, sequence_length)\n",
    "\n",
    "print(f\"  Training sequences created. X_train_sequences shape: {X_train_sequences.shape}\")\n",
    "print(f\"  Training target sequences created. y_train_sequences shape: {y_train_sequences.shape}\")\n",
    "print(f\"  Testing sequences created. X_test_sequences shape: {X_test_sequences.shape}\")\n",
    "print(f\"  Testing target sequences created. y_test_sequences shape: {y_test_sequences.shape}\")\n",
    "\n",
    "print(\"\\nExample of the first training input sequence (X_train_sequences[0]):\")\n",
    "print(X_train_sequences[0])\n",
    "print(\"\\nExample of the first training target value (y_train_sequences[0]):\")\n",
    "print(y_train_sequences[0])\n",
    "\n",
    "print(\"\\nSequence Creation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e730fd2e-1ba9-4cfd-9d55-c0aa760608d1",
   "metadata": {},
   "source": [
    "# 2. Deep Learning Model Implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0ff651-9f80-4856-bcb1-fa9760e0fec0",
   "metadata": {},
   "source": [
    "Building the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12ad01b6-1206-4ae7-a352-a88f26e9373f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence length: 60\n",
      "Number of input features: 6\n",
      "\n",
      "Building LSTM Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">42,800</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,050</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m42,800\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │           \u001b[38;5;34m5,050\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m51\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">47,901</span> (187.11 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m47,901\u001b[0m (187.11 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">47,901</span> (187.11 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m47,901\u001b[0m (187.11 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building GRU Model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,400</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,050</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)                            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m32,400\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │           \u001b[38;5;34m5,050\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m51\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">37,501</span> (146.49 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m37,501\u001b[0m (146.49 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">37,501</span> (146.49 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m37,501\u001b[0m (146.49 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building BiLSTM Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">85,600</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,050</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)                 │          \u001b[38;5;34m85,600\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │          \u001b[38;5;34m10,050\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m51\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">95,701</span> (373.83 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m95,701\u001b[0m (373.83 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">95,701</span> (373.83 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m95,701\u001b[0m (373.83 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deep Learning Models (LSTM, GRU, BiLSTM) defined and compiled!\n",
      "These models are now ready for training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "# Get the shape of input features from our sequences\n",
    "# X_train_sequences shape: (num_samples, sequence_length, num_features)\n",
    "sequence_length = X_train_sequences.shape[1] # sequence_length\n",
    "num_features = X_train_sequences.shape[2] # number of features \n",
    "\n",
    "# Hyperparameters for the models (initial values - these will be tuned later with Bayesian Optimization)\n",
    "lstm_units = 100    # Number of units in the LSTM/GRU/BiLSTM layer\n",
    "dense_units = 50    # Number of units in the subsequent Dense layer\n",
    "dropout_rate = 0.2  # Dropout rate for regularization\n",
    "learning_rate = 0.001 # Learning rate for the Adam optimizer\n",
    "\n",
    "print(f\"Input sequence length: {sequence_length}\")\n",
    "print(f\"Number of input features: {num_features}\")\n",
    "\n",
    "# --- Build the LSTM Model ---\n",
    "def build_lstm_model(seq_len, num_feat, lstm_u, dense_u, dropout_r, lr):\n",
    "    model = Sequential([\n",
    "        # Input Layer: LSTM layer expects input in (batch_size, sequence_length, num_features)\n",
    "        # return_sequences=False by default for the last RNN layer, so it outputs only the last hidden state\n",
    "        LSTM(units=lstm_u, activation='relu', input_shape=(seq_len, num_feat)),\n",
    "        \n",
    "        # Dropout layer for regularization (to prevent overfitting)\n",
    "        Dropout(dropout_r),\n",
    "        \n",
    "        # Dense layer (fully connected layer)\n",
    "        Dense(units=dense_u, activation='relu'),\n",
    "        \n",
    "        # Output layer: Single neuron with no activation for regression (predicting continuous SoC)\n",
    "        Dense(units=1)\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    # Optimizer: Adam is a popular choice\n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    # Loss Function: Mean Squared Error (MSE)\n",
    "    loss_fn = 'mean_squared_error' \n",
    "    # Metrics: Root Mean Squared Error (RMSE) for evaluation, as mentioned in the paper\n",
    "    metrics = [RootMeanSquaredError(name='rmse')]\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "print(\"\\nBuilding LSTM Model...\")\n",
    "lstm_model = build_lstm_model(sequence_length, num_features, lstm_units, dense_units, dropout_rate, learning_rate)\n",
    "lstm_model.summary() # Print a summary of the model's layers and parameters\n",
    "\n",
    "# --- Build the GRU Model ---\n",
    "def build_gru_model(seq_len, num_feat, gru_u, dense_u, dropout_r, lr):\n",
    "    model = Sequential([\n",
    "        GRU(units=gru_u, activation='relu', input_shape=(seq_len, num_feat)),\n",
    "        Dropout(dropout_r),\n",
    "        Dense(units=dense_u, activation='relu'),\n",
    "        Dense(units=1)\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    loss_fn = 'mean_squared_error'\n",
    "    metrics = [RootMeanSquaredError(name='rmse')]\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "print(\"\\nBuilding GRU Model...\")\n",
    "gru_model = build_gru_model(sequence_length, num_features, lstm_units, dense_units, dropout_rate, learning_rate) # Reusing lstm_units for GRU_u\n",
    "gru_model.summary()\n",
    "\n",
    "# --- Build the BiLSTM Model ---\n",
    "def build_bilstm_model(seq_len, num_feat, bilstm_u, dense_u, dropout_r, lr):\n",
    "    model = Sequential([\n",
    "        # Bidirectional wrapper allows LSTM to process sequences in both forward and backward directions\n",
    "        Bidirectional(LSTM(units=bilstm_u, activation='relu', return_sequences=False), input_shape=(seq_len, num_feat)),\n",
    "        Dropout(dropout_r),\n",
    "        Dense(units=dense_u, activation='relu'),\n",
    "        Dense(units=1)\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    loss_fn = 'mean_squared_error'\n",
    "    metrics = [RootMeanSquaredError(name='rmse')]\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "print(\"\\nBuilding BiLSTM Model...\")\n",
    "bilstm_model = build_bilstm_model(sequence_length, num_features, lstm_units, dense_units, dropout_rate, learning_rate) # Reusing lstm_units for BiLSTM_u\n",
    "bilstm_model.summary()\n",
    "\n",
    "print(\"\\nDeep Learning Models (LSTM, GRU, BiLSTM) defined and compiled!\")\n",
    "print(\"These models are now ready for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b2e412-316f-4455-8ea1-2fbd0b1ff166",
   "metadata": {},
   "source": [
    "# 3. Bayesian Optimization for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01ecb61-389e-4ece-831d-0c3c1002271a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Bayesian Optimization for LSTM Model using Optuna...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:27:38,736] A new study created in memory with name: no-name-ae119690-6fb9-4fd1-94f4-6eedcc7ee3a2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X_train_sequences shape: (1222843, 60, 6)\n",
      "X_train_tune shape: (978274, 60, 6), y_train_tune shape: (978274,)\n",
      "X_val_tune shape: (244569, 60, 6), y_val_tune shape: (244569,)\n",
      "\n",
      "Defining Objective Function for GRU Model...\n",
      "\n",
      "Defining Objective Function for BiLSTM Model...\n",
      "\n",
      "Running Optuna study for LSTM model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3fa5ee495d484fbf55d19b045d3b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 21:33:11,819] Trial 0 finished with value: 0.14906355738639832 and parameters: {'lstm_units': 50, 'dense_units': 100, 'dropout_rate': 0.34044600469728353, 'learning_rate': 0.001331121608073689, 'batch_size': 128, 'epochs': 14}. Best is trial 0 with value: 0.14906355738639832.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from sklearn.model_selection import train_test_split # To create a validation set for tuning\n",
    "import optuna\n",
    "import numpy as np # Already imported, but good to keep track\n",
    "\n",
    "\n",
    "print(\"Starting Bayesian Optimization for LSTM Model using Optuna...\")\n",
    "\n",
    "# --- Prepare Data for Hyperparameter Tuning ---\n",
    "# Split the *training* data further into training and validation sets for Optuna.\n",
    "# The test_size here refers to the proportion of the original training data that will be used for validation.\n",
    "X_train_tune, X_val_tune, y_train_tune, y_val_tune = train_test_split(\n",
    "    X_train_sequences, y_train_sequences, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Original X_train_sequences shape: {X_train_sequences.shape}\")\n",
    "print(f\"X_train_tune shape: {X_train_tune.shape}, y_train_tune shape: {y_train_tune.shape}\")\n",
    "print(f\"X_val_tune shape: {X_val_tune.shape}, y_val_tune shape: {y_val_tune.shape}\")\n",
    "\n",
    "\n",
    "# --- Define the Objective Function for Optuna (for LSTM) ---\n",
    "def objective_lstm(trial):\n",
    "    # --- 1. Suggest Hyperparameters ---\n",
    "    lstm_units = trial.suggest_categorical('lstm_units', [30, 50, 70, 100]) # Number of units in LSTM layer\n",
    "    dense_units = trial.suggest_categorical('dense_units', [30, 50, 70, 100]) # Number of units in Dense layer\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5) # Dropout regularization\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True) # Log-uniform distribution\n",
    "    batch_size = trial.suggest_categorical('batch_size', [64, 128, 256]) # Batch size for training\n",
    "    epochs = trial.suggest_int('epochs', 10, 30) # Number of training epochs (will use callbacks for early stopping)\n",
    "\n",
    "    # --- 2. Build the Model with Suggested Hyperparameters ---\n",
    "    model = Sequential([\n",
    "        LSTM(units=lstm_units, activation='relu', input_shape=(sequence_length, num_features)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(units=dense_units, activation='relu'),\n",
    "        Dense(units=1) # Output layer for regression\n",
    "    ])\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[RootMeanSquaredError(name='rmse')])\n",
    "\n",
    "    # --- 3. Train the Model ---\n",
    "    # Use EarlyStopping to stop training if validation loss doesn't improve\n",
    "    # This prevents overfitting and saves time during hyperparameter search\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=5, restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Fit the model (train)\n",
    "    history = model.fit(\n",
    "        X_train_tune, y_train_tune,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val_tune, y_val_tune),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0 # Set to 0 to suppress verbose output during tuning\n",
    "    )\n",
    "\n",
    "    # --- 4. Evaluate and Return Performance ---\n",
    "    # Get the best validation RMSE achieved during training (from early stopping)\n",
    "    val_rmse = history.history['val_rmse'][-1] # Last value is from the best epoch\n",
    "    \n",
    "    # Pruning: If the trial is not promising, tell Optuna to stop it early.\n",
    "    # This further speeds up the optimization.\n",
    "    trial.report(val_rmse, trial.number) # Report current RMSE to Optuna\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return val_rmse # Optuna aims to minimize this value\n",
    "\n",
    "\n",
    "# --- Define the Objective Function for Optuna (for GRU) ---\n",
    "print(\"\\nDefining Objective Function for GRU Model...\")\n",
    "\n",
    "def objective_gru(trial):\n",
    "    # --- 1. Suggest Hyperparameters for GRU ---\n",
    "    gru_units = trial.suggest_categorical('gru_units', [30, 50, 70, 100])\n",
    "    dense_units = trial.suggest_categorical('dense_units', [30, 50, 70, 100])\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [64, 128, 256])\n",
    "    epochs = trial.suggest_int('epochs', 10, 30)\n",
    "\n",
    "    # --- 2. Build the GRU Model with Suggested Hyperparameters ---\n",
    "    model = Sequential([\n",
    "        GRU(units=gru_units, activation='relu', input_shape=(sequence_length, num_features)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(units=dense_units, activation='relu'),\n",
    "        Dense(units=1) # Output layer for regression\n",
    "    ])\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[RootMeanSquaredError(name='rmse')])\n",
    "\n",
    "    # --- 3. Train the Model ---\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=5, restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_tune, y_train_tune,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val_tune, y_val_tune),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # --- 4. Evaluate and Return Performance ---\n",
    "    val_rmse = history.history['val_rmse'][-1]\n",
    "    trial.report(val_rmse, trial.number)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return val_rmse\n",
    "\n",
    "# --- Define the Objective Function for Optuna (for BiLSTM) ---\n",
    "print(\"\\nDefining Objective Function for BiLSTM Model...\")\n",
    "\n",
    "def objective_bilstm(trial):\n",
    "    # --- 1. Suggest Hyperparameters for BiLSTM ---\n",
    "    bilstm_units = trial.suggest_categorical('bilstm_units', [30, 50, 70, 100]) # Units for the LSTM layers within Bidirectional\n",
    "    dense_units = trial.suggest_categorical('dense_units', [30, 50, 70, 100])\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [64, 128, 256])\n",
    "    epochs = trial.suggest_int('epochs', 20, 100)\n",
    "\n",
    "    # --- 2. Build the BiLSTM Model with Suggested Hyperparameters ---\n",
    "    model = Sequential([\n",
    "        # Bidirectional wrapper around an LSTM layer\n",
    "        Bidirectional(LSTM(units=bilstm_units, activation='relu', return_sequences=False), \n",
    "                      input_shape=(sequence_length, num_features)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(units=dense_units, activation='relu'),\n",
    "        Dense(units=1) # Output layer for regression\n",
    "    ])\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[RootMeanSquaredError(name='rmse')])\n",
    "\n",
    "    # --- 3. Train the Model ---\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=5, restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_tune, y_train_tune,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val_tune, y_val_tune),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # --- 4. Evaluate and Return Performance ---\n",
    "    val_rmse = history.history['val_rmse'][-1]\n",
    "    trial.report(val_rmse, trial.number)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return val_rmse\n",
    "\n",
    "\n",
    "\n",
    "# --- Run the Optuna Optimization Study ---\n",
    "# Create an Optuna study object. 'direction=\"minimize\"' means we want to find the minimum RMSE.\n",
    "print(\"\\nRunning Optuna study for LSTM model...\")\n",
    "# You can use a database to store results and resume studies, or 'InMemoryStorage()' for simple use\n",
    "study_lstm = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42)) \n",
    "# 'n_trials' specifies how many different hyperparameter combinations Optuna will try\n",
    "study_lstm.optimize(objective_lstm, n_trials=20, show_progress_bar=True) # Run for 20 trials\n",
    "\n",
    "print(\"\\nBayesian Optimization for LSTM Model Complete!\")\n",
    "print(f\"Best trial for LSTM model: {study_lstm.best_trial.value:.4f} RMSE\")\n",
    "print(\"Best hyperparameters found for LSTM:\")\n",
    "print(study_lstm.best_params)\n",
    "\n",
    "# Store the best parameters for later use\n",
    "best_lstm_params = study_lstm.best_params\n",
    "\n",
    "# --- Run Optuna Optimization Study for GRU Model ---\n",
    "print(\"\\nRunning Optuna study for GRU model (this may take some time)...\")\n",
    "study_gru = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_gru.optimize(objective_gru, n_trials=20, show_progress_bar=True) # Adjust n_trials as needed\n",
    "\n",
    "print(\"\\nBayesian Optimization for GRU Model Complete!\")\n",
    "print(f\"Best trial for GRU model: {study_gru.best_trial.value:.4f} RMSE\")\n",
    "print(\"Best hyperparameters found for GRU:\")\n",
    "print(study_gru.best_params)\n",
    "best_gru_params = study_gru.best_params\n",
    "\n",
    "# --- Run Optuna Optimization Study for BiLSTM Model ---\n",
    "print(\"\\nRunning Optuna study for BiLSTM model (this may take some time)...\")\n",
    "study_bilstm = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_bilstm.optimize(objective_bilstm, n_trials=20, show_progress_bar=True) # Adjust n_trials as needed\n",
    "\n",
    "print(\"\\nBayesian Optimization for BiLSTM Model Complete!\")\n",
    "print(f\"Best trial for BiLSTM model: {study_bilstm.best_trial.value:.4f} RMSE\")\n",
    "print(\"Best hyperparameters found for BiLSTM:\")\n",
    "print(study_bilstm.best_params)\n",
    "best_bilstm_params = study_bilstm.best_params\n",
    "\n",
    "print(\"\\nAll Bayesian Optimization studies for LSTM, GRU, and BiLSTM are now defined and can be run.\")\n",
    "print(\"The 'best_lstm_params', 'best_gru_params', and 'best_bilstm_params' variables now hold the optimal hyperparameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aa8a43-51ee-4859-94e9-26fbf54dc97e",
   "metadata": {},
   "source": [
    "# 4. Final Model Training and Evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b604c6a7-a017-404b-8604-8d126caf8ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "import numpy as np\n",
    "import pandas as pd # For creating a summary table\n",
    "\n",
    "\n",
    "\n",
    "# Placeholder for best_params if Optuna was not run in the same session:\n",
    "if 'best_lstm_params' not in locals():\n",
    "    print(\"Warning: best_lstm_params not found. Using default values.\")\n",
    "    best_lstm_params = {\n",
    "        'lstm_units': 100, 'dense_units': 50, 'dropout_rate': 0.2,\n",
    "        'learning_rate': 0.001, 'batch_size': 32, 'epochs': 50\n",
    "    }\n",
    "if 'best_gru_params' not in locals():\n",
    "    print(\"Warning: best_gru_params not found. Using default values.\")\n",
    "    best_gru_params = {\n",
    "        'gru_units': 100, 'dense_units': 50, 'dropout_rate': 0.2,\n",
    "        'learning_rate': 0.001, 'batch_size': 32, 'epochs': 50\n",
    "    }\n",
    "if 'best_bilstm_params' not in locals():\n",
    "    print(\"Warning: best_bilstm_params not found. Using default values.\")\n",
    "    best_bilstm_params = {\n",
    "        'bilstm_units': 100, 'dense_units': 50, 'dropout_rate': 0.2,\n",
    "        'learning_rate': 0.001, 'batch_size': 32, 'epochs': 50\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"\\n--- Step 4: Final Model Training and Evaluation ---\")\n",
    "\n",
    "# --- Helper Function to Build, Train, and Evaluate Models ---\n",
    "def train_and_evaluate_model(model_type, params, X_train, y_train, X_test, y_test, \n",
    "                             seq_len, num_feat):\n",
    "    \"\"\"\n",
    "    Builds, trains, and evaluates a deep learning model with given parameters.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Training and Evaluating {model_type} Model ---\")\n",
    "    print(f\"Using parameters: {params}\")\n",
    "\n",
    "    # Build the model based on type\n",
    "    model = Sequential()\n",
    "    if model_type == 'LSTM':\n",
    "        model.add(LSTM(units=params['lstm_units'], activation='relu', input_shape=(seq_len, num_feat)))\n",
    "    elif model_type == 'GRU':\n",
    "        model.add(GRU(units=params['gru_units'], activation='relu', input_shape=(seq_len, num_feat)))\n",
    "    elif model_type == 'BiLSTM':\n",
    "        model.add(Bidirectional(LSTM(units=params['bilstm_units'], activation='relu', return_sequences=False), \n",
    "                                input_shape=(seq_len, num_feat)))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model_type. Choose 'LSTM', 'GRU', or 'BiLSTM'.\")\n",
    "\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    model.add(Dense(units=params['dense_units'], activation='relu'))\n",
    "    model.add(Dense(units=1)) # Output layer\n",
    "\n",
    "    optimizer = Adam(learning_rate=params['learning_rate'])\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[RootMeanSquaredError(name='rmse')])\n",
    "\n",
    "    # Callbacks for robust training\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=15, restore_best_weights=True, mode='min' # Increased patience slightly\n",
    "    )\n",
    "    reduce_lr_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6, verbose=0 # Reduce LR if no improvement\n",
    "    )\n",
    "\n",
    "    # Train the model on the full training dataset\n",
    "    print(\"Training model...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=params['epochs'], # Max epochs, EarlyStopping will stop it earlier\n",
    "        batch_size=params['batch_size'],\n",
    "        validation_split=0.1, # Use a small split of the training data for monitoring validation loss during training\n",
    "        callbacks=[early_stopping, reduce_lr_on_plateau],\n",
    "        verbose=1 # Show training progress\n",
    "    )\n",
    "\n",
    "    print(\"\\nEvaluating model on test set...\")\n",
    "    # Evaluate on the unseen test set\n",
    "    test_loss, test_rmse = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"{model_type} Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "    # Predict on the test set to calculate Max Error\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate Max Error (absolute difference between true and predicted values)\n",
    "    max_error = np.max(np.abs(y_test - y_pred.flatten())) # Flatten y_pred to match y_test shape\n",
    "    print(f\"{model_type} Max Error: {max_error:.4f}\")\n",
    "    \n",
    "    return {'RMSE': test_rmse, 'Max Error': max_error, 'Model': model}\n",
    "\n",
    "\n",
    "# --- Train and Evaluate Each Model ---\n",
    "results = {}\n",
    "\n",
    "# LSTM Model\n",
    "lstm_result = train_and_evaluate_model(\n",
    "    'LSTM', best_lstm_params, X_train_sequences, y_train_sequences, \n",
    "    X_test_sequences, y_test_sequences, sequence_length, num_features\n",
    ")\n",
    "results['LSTM'] = lstm_result\n",
    "\n",
    "# GRU Model\n",
    "gru_result = train_and_evaluate_model(\n",
    "    'GRU', best_gru_params, X_train_sequences, y_train_sequences, \n",
    "    X_test_sequences, y_test_sequences, sequence_length, num_features\n",
    ")\n",
    "results['GRU'] = gru_result\n",
    "\n",
    "# BiLSTM Model\n",
    "bilstm_result = train_and_evaluate_model(\n",
    "    'BiLSTM', best_bilstm_params, X_train_sequences, y_train_sequences, \n",
    "    X_test_sequences, y_test_sequences, sequence_length, num_features\n",
    ")\n",
    "results['BiLSTM'] = bilstm_result\n",
    "\n",
    "# --- Summarize Results ---\n",
    "print(\"\\n--- Summary of Model Performance ---\")\n",
    "summary_data = []\n",
    "for model_name, res in results.items():\n",
    "    summary_data.append({\n",
    "        'Model': model_name,\n",
    "        'Test RMSE': res['RMSE'],\n",
    "        'Test Max Error': res['Max Error']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(summary_data)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nFinal models trained and evaluated. The results above show their performance on the unseen test set.\")\n",
    "print(\"This completes the replication process of the paper's methodology.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e37cb0-0bda-4dfd-8746-a1f138574484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e2329e-2451-4a3b-8017-b1fcef7d3c74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
